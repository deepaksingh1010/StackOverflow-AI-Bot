{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries for creating structured data types and handling variables that may not always have a value.\n",
    "from dataclasses import dataclass, field\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "# Defining a class to organize the settings and options for our script, complete with explanations for each setting.\n",
    "@dataclass\n",
    "class ScriptArguments:\n",
    "    # Token needed to access Hugging Face services, like downloading models or datasets.\n",
    "    hf_token: str = field(metadata={\"help\": \"Token for Hugging Face services, necessary for access to models or datasets.\"})\n",
    "\n",
    "    # Name or path to the machine learning model we're starting with, defaulting to a version of Llama.\n",
    "    model_name: Optional[str] = field(\n",
    "        default=\"meta-llama/Llama-2-7b-hf\", metadata={\"help\": \"Identifier for the initial machine learning model, with a default option provided.\"}\n",
    "    )\n",
    "\n",
    "    # Number used to initialize random processes, allowing for consistent outcomes across runs.\n",
    "    seed: Optional[int] = field(\n",
    "        default=4761, metadata={'help': 'Initial value for random processes, ensuring consistent results across different runs.'}\n",
    "    )\n",
    "\n",
    "    # Location or name of the dataset to be used for model training, indicating where the training data comes from.\n",
    "    data_path: Optional[str] = field(\n",
    "        default=\"jbrophy123/stackoverflow_dataset\", metadata={\"help\": \"Location or name of the dataset for training, indicating the source of training data.\"}\n",
    "    )\n",
    "\n",
    "    # Folder where training outputs like model snapshots are saved, useful for monitoring progress and resuming training.\n",
    "    output_dir: Optional[str] = field(\n",
    "        default=\"output\", metadata={\"help\": \"Folder for saving outputs from training, such as snapshots of the model at various stages.\"}\n",
    "    )\n",
    "    \n",
    "    # Number of data examples seen by the model before it updates its learning, influencing the speed and memory requirements of training.\n",
    "    per_device_train_batch_size: Optional[int] = field(\n",
    "        default=2, metadata={\"help\": \"The count of data samples evaluated per device prior to each learning update, balancing between training speed and memory use.\"}\n",
    "    )\n",
    "\n",
    "    # Technique for dealing with memory limits by spreading out gradient updates over several smaller sets of data.\n",
    "    gradient_accumulation_steps: Optional[int] = field(\n",
    "        default=1, metadata={\"help\": \"Method for managing memory limits by distributing updates over smaller batches.\"}\n",
    "    )\n",
    "\n",
    "    # The method used to refine the model weights to reduce prediction errors, with variations impacting the efficiency of the training process.\n",
    "    optim: Optional[str] = field(\n",
    "        default=\"paged_adamw_32bit\", metadata={\"help\": \"The technique for optimizing model weights to lessen prediction errors, varying in efficiency.\"}\n",
    "    )\n",
    "\n",
    "    # Frequency at which the model state is saved during training, aiding in tracking progress and recovery from stops.\n",
    "    save_steps: Optional[int] = field(\n",
    "        default=25, metadata={\"help\": \"How often the model's state is saved, aiding in progress tracking and recovery.\"}\n",
    "    )\n",
    "\n",
    "    # Regularity of logging training progress, assisting in evaluating the model's learning over time.\n",
    "    logging_steps: Optional[int] = field(\n",
    "        default=1, metadata={\"help\": \"Frequency of progress reports during training, helping evaluate learning over time.\"}\n",
    "    )\n",
    "\n",
    "    # Rate at which adjustments are made to the model's weights during training, crucial for the model's learning trajectory.\n",
    "    learning_rate: Optional[float] = field(\n",
    "        default=2e-4, metadata={\"help\": \"Adjustment rate for the model's weights, critical for determining the learning path.\"}\n",
    "    )\n",
    "\n",
    "    # Strategy to prevent excessively large updates during training, maintaining stability and reliability in the learning process.\n",
    "    max_grad_norm: Optional[float] = field(\n",
    "        default=0.3, metadata={\"help\": \"Limit on the size of updates during training, ensuring stability in learning.\"}\n",
    "    )\n",
    "\n",
    "    # Total number of times the training dataset is passed through the model, influencing the depth of learning.\n",
    "    num_train_epochs: Optional[int] = field(\n",
    "        default=1, metadata={\"help\": \"Total dataset iterations for training, affecting the depth of model learning.\"}\n",
    "    )\n",
    "\n",
    "    # Initial training phase where the learning rate increases from zero, enhancing model stability and early performance.\n",
    "    warmup_ratio: Optional[float] = field(\n",
    "        default=0.03, metadata={\"help\": \"Initial phase of increasing learning rate from zero, boosting early training performance.\"}\n",
    "    )\n",
    "\n",
    "    # System for modifying the learning rate throughout training, optimizing the training process and end results.\n",
    "    lr_scheduler_type: Optional[str] = field(\n",
    "        default=\"cosine\", metadata={\"help\": \"System for dynamic learning rate adjustments, optimizing the training outcome.\"}\n",
    "    )\n",
    "\n",
    "    # Location for saving or accessing model parameters enhanced with Low-Rank Adaptation, facilitating model improvements.\n",
    "    lora_dir: Optional[str] = field(default=\"savfav/stackoverflow\", metadata\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
