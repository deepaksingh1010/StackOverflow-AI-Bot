{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and modules for file system operations, machine learning models, and deep learning.\n",
    "import os  # For interacting with the operating system, like handling file paths.\n",
    "import transformers  # Core library for machine learning model operations.\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,  # Loads models for text generation automatically.\n",
    "    AutoTokenizer,  # Loads a tokenizer to convert text to a format the model can process.\n",
    "    set_seed,  # Initializes the random number generator to a fixed value for reproducibility.\n",
    "    BitsAndBytesConfig,  # Configures the model for low-memory training using 4-bit precision.\n",
    "    Trainer,  # Simplifies model training with built-in functions.\n",
    "    TrainingArguments,  # Customizes training with specific parameters.\n",
    "    HfArgumentParser  # Processes command-line arguments for scripts, tailored for Hugging Face models.\n",
    ")\n",
    "from datasets import load_dataset  # Fetches datasets from either Hugging Face Hub or locally.\n",
    "import torch  # Fundamental package for deep learning.\n",
    "\n",
    "import bitsandbytes as bnb  # Enhances training efficiency, particularly with 8-bit optimizers.\n",
    "from huggingface_hub import login, HfFolder  # Manages interactions with the Hugging Face Hub, such as model saving.\n",
    "\n",
    "from trl import SFTTrainer  # Specialized trainer for specific training methodologies.\n",
    "\n",
    "from utils import print_trainable_parameters, find_all_linear_names  # Helpers for analyzing and setting up the model.\n",
    "\n",
    "from train_args import ScriptArguments  # Loads predefined training settings.\n",
    "\n",
    "# Read and apply training configurations from an external script.\n",
    "parser = HfArgumentParser(ScriptArguments)\n",
    "args = parser.parse_args_into_dataclasses()[0]\n",
    "\n",
    "# Main function that orchestrates the model training process.\n",
    "def training_function(args):\n",
    "    \n",
    "    login(token=args.hf_token)  # Securely logs into Hugging Face using a provided token.\n",
    "\n",
    "    set_seed(args.seed)  # Fixes the seed for random number generation to ensure repeatable results.\n",
    "\n",
    "    data_path=args.data_path  # Defines the path to the dataset.\n",
    "\n",
    "    dataset = load_dataset(data_path)  # Retrieves the dataset specified by the path.\n",
    "    \n",
    "    # Sets up the model to train with 4-bit weights, reducing memory footprint.\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    # Initializes the model with the specified configuration for efficient training.\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_name,\n",
    "        use_cache=False,  # Disables caching to conserve disk space.\n",
    "        device_map=\"auto\",  # Selects the best hardware configuration automatically.\n",
    "        quantization_config=bnb_config,  # Applies memory-saving configuration.\n",
    "        trust_remote_code=True  # Allows execution of custom code from the model's Hugging Face repository.\n",
    "    )\n",
    "\n",
    "    # Prepares the tokenizer that converts text into a format the model understands.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "    tokenizer.pad_token=tokenizer.eos_token  # Sets the padding token to the end-of-sequence token for consistency.\n",
    "    tokenizer.padding_side='right'  # Ensures padding is applied to the right, aligning with model expectations.\n",
    "\n",
    "    model = prepare_model_for_kbit_training(model)  # Adjusts the model for training with reduced precision weights.\n",
    "        \n",
    "    # Identifies parts of the model eligible for Low-Rank Adaptation, focusing on efficient fine-tuning.\n",
    "    modules = find_all_linear_names(model)\n",
    "    config = LoraConfig(\n",
    "        r=64,  # Specifies the dimensionality reduction for more efficient computation.\n",
    "        lora_alpha=16,  # Adjusts the impact of the low-rank updates on the model.\n",
    "        lora_dropout=0.1,  # Applies a dropout rate to prevent overfitting by ignoring parts of the updates.\n",
    "        bias='none',  # Omits bias terms from the low-rank layers for simplicity.\n",
    "        task_type='CAUSAL_LM',  # Indicates the model's use case, here text generation.\n",
    "        target_modules=modules  # Targets specific model components for adaptation.\n",
    "    )\n",
    "\n",
    "    # Applies the LoRA settings to the model, preparing it for targeted fine-tuning.\n",
    "    model = get_peft_model(model, config)\n",
    "    \n",
    "    # Determines the directory where the trained model will be saved.\n",
    "    output_dir = args.output_dir\n",
    "\n",
    "    # Configures training parameters, such as batch size, learning rate, and training epochs.\n",
    "    training_arguments = TrainingArguments(\n",
    "        output_dir=output_dir,  # Designates the save location for the trained model.\n",
    "        per_device_train_batch_size=args.per_device_train_batch_size,  # Sets the number of samples per update per device.\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps,  #\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
